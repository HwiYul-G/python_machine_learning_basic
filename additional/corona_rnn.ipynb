{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 만드는 AI는 \n",
    "# 이전 3일(일수 변경 가능)의 확진자 수 추이를 보고 다음날의 확진자 수를 예측\n",
    "\n",
    "# 연속된 데이터의 형태에서 그 패턴을 찾아내는 순환 신경망(RNN) 방식으로, \n",
    "# RNN의 기본적인 형탤르 설계하고 학습시켜본다.\n",
    "\n",
    "#  케라스의 모델 도구 중 시퀀셜 모델 불러옴\n",
    "from keras.models import Sequential\n",
    "\n",
    "# 순환신경망(RNN) 기법에는 LSTM, GRU 등 다양한 기법이 있다.\n",
    "# SimpleRNN은 가장 기본적인 RNN의 모습으로 LSTM과 GRU는 SimpleRNN을 한층 더 발전시킨 순환 신경망임\n",
    "# Dense는 각 레이어에서 뉴런의 수이다. 각 레이어에 들어가는 뉴런의 수를 정할 때 사용한다.\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "\n",
    "# 데이터 정규화를 위한 sklearn 라이브러리의 전처리 함수 사용\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 결과 정확도를 계산하기 위한 함수를 불러옴\n",
    "# 코로나 확진자 수를 예측하는 모델의 결과는 특정한 숫자로 나온다.\n",
    "# 연속된 값을 예측하는 회귀 문제로 오차를 게산하는 방법 또한 분류 문제와 다르다.\n",
    "# 이때 mean_squared_error로 실제 값과 예측 값의 차이를 사용해 오류를 구하는 역할\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 트레이닝 데이터와 테스트 데이터를 나누는 명령어\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Confirmed\n",
      "0           24\n",
      "1           24\n",
      "2           27\n",
      "3           27\n",
      "4           28\n",
      "..         ...\n",
      "107      11190\n",
      "108      11206\n",
      "109      11225\n",
      "110      11265\n",
      "111      11344\n",
      "\n",
      "[112 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 가져오기\n",
    "# git에 저자분이 올려주신 것을 불러옴 !git clone https://github.com/yhlee1627/deeplearning.git 을 이용해 먼저 불러오는 것을 수행해야 한다.\n",
    "\n",
    "# 확진자 수만 사용해 모델을 생성할 것이므로 필요한 3번 컬럼만 받아옴 usecols=[3]으로\n",
    "dataframe = read_csv('../data/corona_daily.csv', usecols=[3], engine='python', skipfooter=3)\n",
    "\n",
    "print(dataframe)\n",
    "\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32') # 정규화를 위해 두 번째 행의 값을 실수로 변경 (정규화는 보통 나눗셈을 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 23\n"
     ]
    }
   ],
   "source": [
    "# 데이터 정규화 및 분류하기\n",
    "# 인공지능 모델의 성능을 높이려면 데이터 정규화가 필요하다. 여기선 데이털르 0과 1사이로 만들어 사용\n",
    "\n",
    "# 정규화하기 위한 방법을 scaler로 정하고, 이를 위해 사이킷런 라이브러리 중 MinMaxScaler 함수를 사용\n",
    "scaler = MinMaxScaler(feature_range=(0, 1)) # 데이터 정규화 범이를 0, 1로 정함\n",
    "Dataset = scaler.fit_transform(dataset) # 앞에서 만든 정규화 방법인 scaler를 사용한 후, MinMaxScaler 함수 중 fit_transform 함수를 사용해 데이터를 정규화함\n",
    "train_data, test_data = train_test_split(Dataset, test_size=0.2, shuffle=False)\n",
    "\n",
    "# 훈련데이터의 개수와 검증 데이터의 개수를 출력\n",
    "print(len(train_data), len(test_data)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터의 형태 변경하기\n",
    "\n",
    "# RNN(순환 신경망) 모델은 이전의 연속된 데이털르 사용해서 이후의 값을 예측한다.\n",
    "# 1, 2, 3일차의 확진자 수(연속된 데이터)를 순환 신경망 모델에 넣으면 그 다음 날짜의 확진자 수, 즉 4일차 확진자 수를 예측해서 반환해 준다.\n",
    "# 그리고 7, 8, 9일차를 넣으면 10일차를 예측해서 반환한다.\n",
    "\n",
    "# 이러한 형태의 예측을 위해선 데이터의 모습 또한 이에 맞게 변경해야 한다.\n",
    "# 그런데 우리가 가진 데이터는 한 줄로 쭉 나열된 모습이다. 따라서 인공지능 모델에 데이터를 입력하기 위해선 형태를 변경해야 한다.\n",
    "\n",
    "# arg1 : 원래 데이터 arg2 : 연속되는 데이터의 개수\n",
    "def create_dataset(dataset, look_back):\n",
    "    x_data, y_data = [], []\n",
    "    for i in range(len(dataset)-look_back):\n",
    "        data = dataset[i:(i+look_back), 0]\n",
    "        x_data.append(data)\n",
    "        y_data.append(dataset[i + look_back, 0])\n",
    "    return np.array(x_data), np.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 3) (86,)\n",
      "(20, 3) (20,)\n"
     ]
    }
   ],
   "source": [
    "# create input data\n",
    "look_back = 3\n",
    "x_train, y_train = create_dataset(train_data, look_back)\n",
    "x_test, y_test = create_dataset(test_data, look_back)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 1, 3)\n",
      "(20, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "# 인공지능 모델에 넣어줄 형태로 변환하기\n",
    "# 현재 가진 것은 row 85, col 3인데 -> 인공지능에 넣을 땐 하나씩 따로 넣어야해서 1x3 형태 85개, 즉 85x1x3\n",
    "\n",
    "X_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "X_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 3)                 21        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 25\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 인공지능 모델 만들기\n",
    "# 일반적인 시퀀셜 모델은 입력데이터가 은닉층을 거쳐 출력층까지 간다.\n",
    "\n",
    "# 순환 신경망은 첫 번째 데이터를 넣고 은닉층에 있는 파라미터들(가중치와 편형의 값)을 학습\n",
    "# 그 학습 결괏값을 바로 출력하는 것이 아니라 다음 단계에서 참고할 수 있도록 넘겨줌\n",
    "# 이후 똑같은 은닉층에 첫 번째 데이터를 넣고 학습한 결과와 함께 두 번째 데이털르 넣고 학습시킨다.\n",
    "# 이때는 앞에서 첫 번쨰 값을 넣었을 때 학습한 결괏값을 포함해 학습을 시작하고, 다음 이 결과를 다시 다음 단계로 넘김\n",
    "# 이후 이 결괏값과 세 번째 데이터를 넣고 학습시킨 후 최종값을 예측하는 구조\n",
    "\n",
    "# RNN 역시 레이어들이 선형으로 연결되므로 시퀀셜 모델로\n",
    "model = Sequential()\n",
    "\n",
    "# RNN 기법 중 Simple RNN을 사용 (LSTM, GRU 등 다양한 기법이 있다.)\n",
    "# 은닉층의 수는 3개 (랜덤하게 해도 됨), 넣는 데이터의 형태는 1x3의 형태를 넣음\n",
    "model.add(SimpleRNN(3, input_shape = (1, look_back) ))\n",
    "\n",
    "# 은닉층은 최종적으로 1개만 있으면 되고 l활성화 함수는 linear\n",
    "model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "# 인공지능을 계산하는 방법을 결정\n",
    "# 손실함수는 mse(평균 제곱 오차, mean_squared_error)로, 옵티마이저는 adam 옵티마이저를 사용\n",
    "# 실제 확진자의 수와 에측한 값의 차이를 바탕으로 오차를 나타낼 수 있으므로 평균 제곱 오차를 사용함.\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 944us/step - loss: 0.0028\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 938us/step - loss: 7.5213e-04\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 980us/step - loss: 6.7225e-04\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 6.2131e-04\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 943us/step - loss: 6.1103e-04\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 994us/step - loss: 5.9470e-04\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 838us/step - loss: 6.1426e-04\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 746us/step - loss: 5.2816e-04\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 761us/step - loss: 5.9207e-04\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 757us/step - loss: 6.0976e-04\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 856us/step - loss: 5.2916e-04\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 774us/step - loss: 6.4072e-04\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 947us/step - loss: 5.2800e-04\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 5.4690e-04\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 920us/step - loss: 5.5488e-04\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 5.1652e-04\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 985us/step - loss: 4.9975e-04\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 918us/step - loss: 5.0158e-04\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 898us/step - loss: 5.1055e-04\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 853us/step - loss: 5.1828e-04\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 796us/step - loss: 5.0086e-04\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 673us/step - loss: 4.8374e-04\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 643us/step - loss: 5.0301e-04\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 648us/step - loss: 4.8066e-04\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 675us/step - loss: 5.0403e-04\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 716us/step - loss: 4.8412e-04\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 0s 873us/step - loss: 5.1913e-04\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 0s 959us/step - loss: 4.9018e-04\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - 0s 864us/step - loss: 5.2369e-04\n",
      "Epoch 30/100\n",
      "86/86 [==============================] - 0s 922us/step - loss: 4.8293e-04\n",
      "Epoch 31/100\n",
      "86/86 [==============================] - 0s 935us/step - loss: 4.8516e-04\n",
      "Epoch 32/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 4.6601e-04\n",
      "Epoch 33/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 4.4502e-04\n",
      "Epoch 34/100\n",
      "86/86 [==============================] - 0s 696us/step - loss: 4.4366e-04\n",
      "Epoch 35/100\n",
      "86/86 [==============================] - 0s 794us/step - loss: 4.2986e-04\n",
      "Epoch 36/100\n",
      "86/86 [==============================] - 0s 739us/step - loss: 4.6780e-04\n",
      "Epoch 37/100\n",
      "86/86 [==============================] - 0s 666us/step - loss: 4.1669e-04\n",
      "Epoch 38/100\n",
      "86/86 [==============================] - 0s 717us/step - loss: 4.5331e-04\n",
      "Epoch 39/100\n",
      "86/86 [==============================] - 0s 709us/step - loss: 4.6709e-04\n",
      "Epoch 40/100\n",
      "86/86 [==============================] - 0s 831us/step - loss: 4.1481e-04\n",
      "Epoch 41/100\n",
      "86/86 [==============================] - 0s 874us/step - loss: 4.2583e-04\n",
      "Epoch 42/100\n",
      "86/86 [==============================] - 0s 884us/step - loss: 4.5991e-04\n",
      "Epoch 43/100\n",
      "86/86 [==============================] - 0s 828us/step - loss: 4.7629e-04\n",
      "Epoch 44/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 4.3086e-04\n",
      "Epoch 45/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 4.1185e-04\n",
      "Epoch 46/100\n",
      "86/86 [==============================] - 0s 866us/step - loss: 4.2966e-04\n",
      "Epoch 47/100\n",
      "86/86 [==============================] - 0s 905us/step - loss: 4.2194e-04\n",
      "Epoch 48/100\n",
      "86/86 [==============================] - 0s 834us/step - loss: 4.0622e-04\n",
      "Epoch 49/100\n",
      "86/86 [==============================] - 0s 780us/step - loss: 3.7028e-04\n",
      "Epoch 50/100\n",
      "86/86 [==============================] - 0s 667us/step - loss: 4.0887e-04\n",
      "Epoch 51/100\n",
      "86/86 [==============================] - 0s 651us/step - loss: 4.1450e-04\n",
      "Epoch 52/100\n",
      "86/86 [==============================] - 0s 679us/step - loss: 4.2552e-04\n",
      "Epoch 53/100\n",
      "86/86 [==============================] - 0s 682us/step - loss: 3.8384e-04\n",
      "Epoch 54/100\n",
      "86/86 [==============================] - 0s 635us/step - loss: 4.2345e-04\n",
      "Epoch 55/100\n",
      "86/86 [==============================] - 0s 633us/step - loss: 4.1654e-04\n",
      "Epoch 56/100\n",
      "86/86 [==============================] - 0s 881us/step - loss: 3.8701e-04\n",
      "Epoch 57/100\n",
      "86/86 [==============================] - 0s 870us/step - loss: 3.6627e-04\n",
      "Epoch 58/100\n",
      "86/86 [==============================] - 0s 961us/step - loss: 3.8274e-04\n",
      "Epoch 59/100\n",
      "86/86 [==============================] - 0s 857us/step - loss: 4.0510e-04\n",
      "Epoch 60/100\n",
      "86/86 [==============================] - 0s 878us/step - loss: 3.8551e-04\n",
      "Epoch 61/100\n",
      "86/86 [==============================] - 0s 870us/step - loss: 4.0244e-04\n",
      "Epoch 62/100\n",
      "86/86 [==============================] - 0s 845us/step - loss: 3.6450e-04\n",
      "Epoch 63/100\n",
      "86/86 [==============================] - 0s 908us/step - loss: 3.8350e-04\n",
      "Epoch 64/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 4.0842e-04\n",
      "Epoch 65/100\n",
      "86/86 [==============================] - 0s 722us/step - loss: 3.6294e-04\n",
      "Epoch 66/100\n",
      "86/86 [==============================] - 0s 738us/step - loss: 3.5617e-04\n",
      "Epoch 67/100\n",
      "86/86 [==============================] - 0s 713us/step - loss: 3.6305e-04\n",
      "Epoch 68/100\n",
      "86/86 [==============================] - 0s 668us/step - loss: 3.6917e-04\n",
      "Epoch 69/100\n",
      "86/86 [==============================] - 0s 777us/step - loss: 3.6975e-04\n",
      "Epoch 70/100\n",
      "86/86 [==============================] - 0s 818us/step - loss: 3.5800e-04\n",
      "Epoch 71/100\n",
      "86/86 [==============================] - 0s 840us/step - loss: 3.6557e-04\n",
      "Epoch 72/100\n",
      "86/86 [==============================] - 0s 810us/step - loss: 3.3970e-04\n",
      "Epoch 73/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 3.7783e-04\n",
      "Epoch 74/100\n",
      "86/86 [==============================] - 0s 784us/step - loss: 3.4326e-04\n",
      "Epoch 75/100\n",
      "86/86 [==============================] - 0s 862us/step - loss: 3.5732e-04\n",
      "Epoch 76/100\n",
      "86/86 [==============================] - 0s 848us/step - loss: 3.4635e-04\n",
      "Epoch 77/100\n",
      "86/86 [==============================] - 0s 982us/step - loss: 3.5157e-04\n",
      "Epoch 78/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 3.4398e-04\n",
      "Epoch 79/100\n",
      "86/86 [==============================] - 0s 807us/step - loss: 3.2945e-04\n",
      "Epoch 80/100\n",
      "86/86 [==============================] - 0s 662us/step - loss: 3.6786e-04\n",
      "Epoch 81/100\n",
      "86/86 [==============================] - 0s 642us/step - loss: 3.7075e-04\n",
      "Epoch 82/100\n",
      "86/86 [==============================] - 0s 666us/step - loss: 3.4112e-04\n",
      "Epoch 83/100\n",
      "86/86 [==============================] - 0s 731us/step - loss: 3.2711e-04\n",
      "Epoch 84/100\n",
      "86/86 [==============================] - 0s 916us/step - loss: 3.3337e-04\n",
      "Epoch 85/100\n",
      "86/86 [==============================] - 0s 844us/step - loss: 3.2423e-04\n",
      "Epoch 86/100\n",
      "86/86 [==============================] - 0s 965us/step - loss: 3.0296e-04\n",
      "Epoch 87/100\n",
      "86/86 [==============================] - 0s 958us/step - loss: 3.0779e-04\n",
      "Epoch 88/100\n",
      "86/86 [==============================] - 0s 980us/step - loss: 3.4127e-04\n",
      "Epoch 89/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 3.2045e-04\n",
      "Epoch 90/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 3.0768e-04\n",
      "Epoch 91/100\n",
      "86/86 [==============================] - 0s 934us/step - loss: 3.5837e-04\n",
      "Epoch 92/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 3.1623e-04\n",
      "Epoch 93/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 3.1577e-04\n",
      "Epoch 94/100\n",
      "86/86 [==============================] - 0s 784us/step - loss: 3.3922e-04\n",
      "Epoch 95/100\n",
      "86/86 [==============================] - 0s 703us/step - loss: 2.9508e-04\n",
      "Epoch 96/100\n",
      "86/86 [==============================] - 0s 806us/step - loss: 3.1491e-04\n",
      "Epoch 97/100\n",
      "86/86 [==============================] - 0s 764us/step - loss: 2.9262e-04\n",
      "Epoch 98/100\n",
      "86/86 [==============================] - 0s 681us/step - loss: 3.2348e-04\n",
      "Epoch 99/100\n",
      "86/86 [==============================] - 0s 956us/step - loss: 3.0231e-04\n",
      "Epoch 100/100\n",
      "86/86 [==============================] - 0s 919us/step - loss: 2.6838e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21a6803eee0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습시키기\n",
    "# 에포크 : 반복 횟수, batch_size : 한번에 학습할 사이즈 , verbose 1 : 학습의 진행 결과를 에포크별로 간단히 알려줌\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    }
   ],
   "source": [
    "# 데이터 예측하기\n",
    "# 모델의 성능을 측정하려면 실제 데이터를 예측한 값과 실제 데이터의 값의 차이를 봐야한다.\n",
    "# 그러므로 정규화를 거친 결과가 아닌 실제 확진자 수 데이터가 필요하다.\n",
    "# RNN 모델을 통해 나온 예측값을 정규화 되기 전으로 변환해 실제 값 또한 정규화되기 전의 값으로 변회시켜야함\n",
    "\n",
    "# X_train, X_tst -> RNN Model -> RNN -> 예측값     실제값 <= Scaler <- y_train, y_test\n",
    "\n",
    "trainPredict = model.predict(X_train) # 정규화된 값이 나옴\n",
    "testPredict = model.predict(X_test)\n",
    "\n",
    "# 실제 값으로 다시 바꾸기\n",
    "TrainPredict = scaler.inverse_transform(trainPredict)\n",
    "TestPredict = scaler.inverse_transform(testPredict)\n",
    "\n",
    "Y_train = scaler.inverse_transform([y_train])\n",
    "Y_test = scaler.inverse_transform([y_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train : \n",
      " [[   27.00000082    28.00000066    28.00000066    28.00000066\n",
      "     28.00000066    28.00000066    29.00000049    30.00000033\n",
      "     31.00000016    51.00000214   104.00000395   204.00001384\n",
      "    433.00000247   602.00000099   832.99993658   976.9999972\n",
      "   1260.99992933  1766.00008335  2337.00011053  3150.00006622\n",
      "   3736.00006457  4212.00004942  4811.99990841  5327.99992884\n",
      "   5766.00009917  6284.00034066  6766.99991846  7134.00016901\n",
      "   7381.99991731  7512.99972177  7754.99981896  7868.99993724\n",
      "   7979.00028811  8086.0001387   8161.99976773  8236.00018779\n",
      "   8320.00002636  8413.00001631  8564.9999491   8652.00028794\n",
      "   8798.99983675  8897.00021069  8960.99986278  9037.00016654\n",
      "   9136.99974945  9240.99977449  9331.99988074  9478.00016242\n",
      "   9583.00012931  9661.00031677  9785.99979541  9886.99999489\n",
      "   9976.00021744 10061.99993971 10155.99987151 10236.99988453\n",
      "  10283.99985043 10330.99981633 10384.00010806 10422.99986443\n",
      "  10450.00031858 10479.99992357 10512.00008698 10536.9999827\n",
      "  10563.99976213 10591.00021629 10613.00028646 10635.00035664\n",
      "  10652.99998468 10661.00019421 10674.000113   10683.00026439\n",
      "  10694.00029947 10701.99983428 10708.00016012 10718.00025335\n",
      "  10728.00034659 10737.9997651  10752.00030046 10760.99977712\n",
      "  10765.00021925 10774.00037064 10780.00002174 10792.99994053\n",
      "  10801.00015007 10803.99997562]]\n",
      "\n",
      " Y_test: \n",
      "  [[10839.99990643 10873.99995355 10908.99994251 10935.99972194\n",
      "  10962.00023424 10990.99989737 11018.00035153 11036.99992142\n",
      "  11049.99984021 11065.00031743 11078.00023622 11109.9997249\n",
      "  11122.00037657 11141.99988831 11164.99990034 11189.99979607\n",
      "  11206.00021513 11224.99978503 11265.00015797 11344.00028729]]\n"
     ]
    }
   ],
   "source": [
    "print('Y_train : \\n',Y_train)\n",
    "\n",
    "print('\\n Y_test: \\n ', Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score : 229.82 RMSE\n",
      "Test Score : 388.76 RMSE\n"
     ]
    }
   ],
   "source": [
    "# 모델의 정확도 살펴보기\n",
    "# 모델이 예측한 값과 실제값에는 어느정도 차이가 있는 지보기\n",
    "# 이때 사용하는 함수는 평균 제곱근 오차(Root Mean Squared Error)로 구함. 제곱을해서 수가 커지므로 root를 통헤 좀 작게 하는 처리\n",
    "\n",
    "# mean_square_error는 사이킷런 라이브러리에서 가져옴 \n",
    "trainScore = math.sqrt(mean_squared_error(Y_train[0], TrainPredict[:, 0]))\n",
    "print('Train Score : %.2f RMSE' %(trainScore) ) #  출력할 변수를 % 뒤에 넣는다.\n",
    "\n",
    "testScore = math.sqrt(mean_squared_error(Y_test[0], TestPredict[:, 0]))\n",
    "print('Test Score : %.2f RMSE' %(testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 결과를 그래프로 확인하기\n",
    "\n",
    "# RMSE(평균제곱근 오차)를 구해 모델의 정확도를 살펴봤지만, 이 점수만으론 모델의 정확도를 한눈에 파악하기 어렵다.\n",
    "#그래서 실제 데이터의 G와 훈련 데이터를 예측한 그래프, 검증 데이터를 예측한 G를 한 번에 그려서 봄\n",
    "\n",
    "# 훈련 데이터를 예측한 값을 저장할 배열 만들기 \n",
    "trainPredictPlot = np.empty_like(dataset)  # dataset과 동일한 형태의 넘파일 배열을 만든\n",
    "trainPredictPlot[:,:] = np.nan # 모두 값을 nan으로\n",
    "# 3일차를 건너뛰고 4일차(look_back) 부터 TrainPredict 배열의 원소 수까지 len(TrainPredict) + look_back를 범위로 지정\n",
    "# 그 부분에 데이터를 넣음\n",
    "trainPredictPlot[look_back: len(TrainPredict) + look_back, :] = TrainPredict\n",
    "\n",
    "# 초록색(검정)\n",
    "testPredictPlot = np.empty_like(dataset)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "# 뒷 부분에 집어 넣어서 겹치지 않게 하기 위해서\n",
    "# 애초에 TrainPredict그릴때 계산한 lookback + TrainPredict 이후 + look_back*2(앞의 3일을 건너뛴 후 그 후에)\n",
    "testPredictPlot[len(TrainPredict) + (look_back)*2 : len(dataset), : ] = TestPredict \n",
    "\n",
    "plt.plot(dataset)\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()\n",
    "\n",
    "# 파란색 : 데이터 , 주황색 : 훈련, 초록색 : 검증"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7a68ff6df131d9ae7a0f60e2ded381d5113c51d2c465edd8572fa159edd1df8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
