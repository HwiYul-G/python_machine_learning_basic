{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 만드는 AI는 \n",
    "# 이전 3일(일수 변경 가능)의 확진자 수 추이를 보고 다음날의 확진자 수를 예측\n",
    "\n",
    "# 연속된 데이터의 형태에서 그 패턴을 찾아내는 순환 신경망(RNN) 방식으로, \n",
    "# RNN의 기본적인 형탤르 설계하고 학습시켜본다.\n",
    "\n",
    "#  케라스의 모델 도구 중 시퀀셜 모델 불러옴\n",
    "from keras.models import Sequential\n",
    "\n",
    "# 순환신경망(RNN) 기법에는 LSTM, GRU 등 다양한 기법이 있다.\n",
    "# SimpleRNN은 가장 기본적인 RNN의 모습으로 LSTM과 GRU는 SimpleRNN을 한층 더 발전시킨 순환 신경망임\n",
    "# Dense는 각 레이어에서 뉴런의 수이다. 각 레이어에 들어가는 뉴런의 수를 정할 때 사용한다.\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "\n",
    "# 데이터 정규화를 위한 sklearn 라이브러리의 전처리 함수 사용\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 결과 정확도를 계산하기 위한 함수를 불러옴\n",
    "# 코로나 확진자 수를 예측하는 모델의 결과는 특정한 숫자로 나온다.\n",
    "# 연속된 값을 예측하는 회귀 문제로 오차를 게산하는 방법 또한 분류 문제와 다르다.\n",
    "# 이때 mean_squared_error로 실제 값과 예측 값의 차이를 사용해 오류를 구하는 역할\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 트레이닝 데이터와 테스트 데이터를 나누는 명령어\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Confirmed\n",
      "0           24\n",
      "1           24\n",
      "2           27\n",
      "3           27\n",
      "4           28\n",
      "..         ...\n",
      "107      11190\n",
      "108      11206\n",
      "109      11225\n",
      "110      11265\n",
      "111      11344\n",
      "\n",
      "[112 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 가져오기\n",
    "# git에 저자분이 올려주신 것을 불러옴 !git clone https://github.com/yhlee1627/deeplearning.git 을 이용해 먼저 불러오는 것을 수행해야 한다.\n",
    "\n",
    "# 확진자 수만 사용해 모델을 생성할 것이므로 필요한 3번 컬럼만 받아옴 usecols=[3]으로\n",
    "dataframe = read_csv('../data/corona_daily.csv', usecols=[3], engine='python', skipfooter=3)\n",
    "\n",
    "print(dataframe)\n",
    "\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32') # 정규화를 위해 두 번째 행의 값을 실수로 변경 (정규화는 보통 나눗셈을 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 23\n"
     ]
    }
   ],
   "source": [
    "# 데이터 정규화 및 분류하기\n",
    "# 인공지능 모델의 성능을 높이려면 데이터 정규화가 필요하다. 여기선 데이털르 0과 1사이로 만들어 사용\n",
    "\n",
    "# 정규화하기 위한 방법을 scaler로 정하고, 이를 위해 사이킷런 라이브러리 중 MinMaxScaler 함수를 사용\n",
    "scaler = MinMaxScaler(feature_range=(0, 1)) # 데이터 정규화 범이를 0, 1로 정함\n",
    "Dataset = scaler.fit_transform(dataset) # 앞에서 만든 정규화 방법인 scaler를 사용한 후, MinMaxScaler 함수 중 fit_transform 함수를 사용해 데이터를 정규화함\n",
    "train_data, test_data = train_test_split(Dataset, test_size=0.2, shuffle=False)\n",
    "\n",
    "# 훈련데이터의 개수와 검증 데이터의 개수를 출력\n",
    "print(len(train_data), len(test_data)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터의 형태 변경하기\n",
    "\n",
    "# RNN(순환 신경망) 모델은 이전의 연속된 데이털르 사용해서 이후의 값을 예측한다.\n",
    "# 1, 2, 3일차의 확진자 수(연속된 데이터)를 순환 신경망 모델에 넣으면 그 다음 날짜의 확진자 수, 즉 4일차 확진자 수를 예측해서 반환해 준다.\n",
    "# 그리고 7, 8, 9일차를 넣으면 10일차를 예측해서 반환한다.\n",
    "\n",
    "# 이러한 형태의 예측을 위해선 데이터의 모습 또한 이에 맞게 변경해야 한다.\n",
    "# 그런데 우리가 가진 데이터는 한 줄로 쭉 나열된 모습이다. 따라서 인공지능 모델에 데이터를 입력하기 위해선 형태를 변경해야 한다.\n",
    "\n",
    "# arg1 : 원래 데이터 arg2 : 연속되는 데이터의 개수\n",
    "def create_dataset(dataset, look_back):\n",
    "    x_data, y_data = [], []\n",
    "    for i in range(len(dataset)-look_back):\n",
    "        data = dataset[i:(i+look_back), 0]\n",
    "        x_data.append(data)\n",
    "        y_data.append(dataset[i + look_back, 0])\n",
    "    return np.array(x_data), np.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 3) (86,)\n",
      "(20, 3) (20,)\n"
     ]
    }
   ],
   "source": [
    "# create input data\n",
    "look_back = 3\n",
    "x_train, y_train = create_dataset(train_data, look_back)\n",
    "x_test, y_test = create_dataset(test_data, look_back)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 1, 3)\n",
      "(20, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "# 인공지능 모델에 넣어줄 형태로 변환하기\n",
    "# 현재 가진 것은 row 85, col 3인데 -> 인공지능에 넣을 땐 하나씩 따로 넣어야해서 1x3 형태 85개, 즉 85x1x3\n",
    "\n",
    "X_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "X_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 3)                 21        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 25\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 인공지능 모델 만들기\n",
    "# 일반적인 시퀀셜 모델은 입력데이터가 은닉층을 거쳐 출력층까지 간다.\n",
    "\n",
    "# 순환 신경망은 첫 번째 데이터를 넣고 은닉층에 있는 파라미터들(가중치와 편형의 값)을 학습\n",
    "# 그 학습 결괏값을 바로 출력하는 것이 아니라 다음 단계에서 참고할 수 있도록 넘겨줌\n",
    "# 이후 똑같은 은닉층에 첫 번째 데이터를 넣고 학습한 결과와 함께 두 번째 데이털르 넣고 학습시킨다.\n",
    "# 이때는 앞에서 첫 번쨰 값을 넣었을 때 학습한 결괏값을 포함해 학습을 시작하고, 다음 이 결과를 다시 다음 단계로 넘김\n",
    "# 이후 이 결괏값과 세 번째 데이터를 넣고 학습시킨 후 최종값을 예측하는 구조\n",
    "\n",
    "# RNN 역시 레이어들이 선형으로 연결되므로 시퀀셜 모델로\n",
    "model = Sequential()\n",
    "\n",
    "# RNN 기법 중 Simple RNN을 사용 (LSTM, GRU 등 다양한 기법이 있다.)\n",
    "# 은닉층의 수는 3개 (랜덤하게 해도 됨), 넣는 데이터의 형태는 1x3의 형태를 넣음\n",
    "model.add(SimpleRNN(3, input_shape = (1, look_back) ))\n",
    "\n",
    "# 은닉층은 최종적으로 1개만 있으면 되고 l활성화 함수는 linear\n",
    "model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "# 인공지능을 계산하는 방법을 결정\n",
    "# 손실함수는 mse(평균 제곱 오차, mean_squared_error)로, 옵티마이저는 adam 옵티마이저를 사용\n",
    "# 실제 확진자의 수와 에측한 값의 차이를 바탕으로 오차를 나타낼 수 있으므로 평균 제곱 오차를 사용함.\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 944us/step - loss: 0.0028\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 938us/step - loss: 7.5213e-04\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 980us/step - loss: 6.7225e-04\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 6.2131e-04\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 943us/step - loss: 6.1103e-04\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 994us/step - loss: 5.9470e-04\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 838us/step - loss: 6.1426e-04\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 746us/step - loss: 5.2816e-04\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 761us/step - loss: 5.9207e-04\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 757us/step - loss: 6.0976e-04\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 856us/step - loss: 5.2916e-04\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 774us/step - loss: 6.4072e-04\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 947us/step - loss: 5.2800e-04\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 5.4690e-04\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 920us/step - loss: 5.5488e-04\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 5.1652e-04\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 985us/step - loss: 4.9975e-04\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 918us/step - loss: 5.0158e-04\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 898us/step - loss: 5.1055e-04\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 853us/step - loss: 5.1828e-04\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 796us/step - loss: 5.0086e-04\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 673us/step - loss: 4.8374e-04\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 643us/step - loss: 5.0301e-04\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 648us/step - loss: 4.8066e-04\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 675us/step - loss: 5.0403e-04\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 716us/step - loss: 4.8412e-04\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 0s 873us/step - loss: 5.1913e-04\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 0s 959us/step - loss: 4.9018e-04\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - 0s 864us/step - loss: 5.2369e-04\n",
      "Epoch 30/100\n",
      "86/86 [==============================] - 0s 922us/step - loss: 4.8293e-04\n",
      "Epoch 31/100\n",
      "86/86 [==============================] - 0s 935us/step - loss: 4.8516e-04\n",
      "Epoch 32/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 4.6601e-04\n",
      "Epoch 33/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 4.4502e-04\n",
      "Epoch 34/100\n",
      "86/86 [==============================] - 0s 696us/step - loss: 4.4366e-04\n",
      "Epoch 35/100\n",
      "86/86 [==============================] - 0s 794us/step - loss: 4.2986e-04\n",
      "Epoch 36/100\n",
      "86/86 [==============================] - 0s 739us/step - loss: 4.6780e-04\n",
      "Epoch 37/100\n",
      "86/86 [==============================] - 0s 666us/step - loss: 4.1669e-04\n",
      "Epoch 38/100\n",
      "86/86 [==============================] - 0s 717us/step - loss: 4.5331e-04\n",
      "Epoch 39/100\n",
      "86/86 [==============================] - 0s 709us/step - loss: 4.6709e-04\n",
      "Epoch 40/100\n",
      "86/86 [==============================] - 0s 831us/step - loss: 4.1481e-04\n",
      "Epoch 41/100\n",
      "86/86 [==============================] - 0s 874us/step - loss: 4.2583e-04\n",
      "Epoch 42/100\n",
      "86/86 [==============================] - 0s 884us/step - loss: 4.5991e-04\n",
      "Epoch 43/100\n",
      "86/86 [==============================] - 0s 828us/step - loss: 4.7629e-04\n",
      "Epoch 44/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 4.3086e-04\n",
      "Epoch 45/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 4.1185e-04\n",
      "Epoch 46/100\n",
      "86/86 [==============================] - 0s 866us/step - loss: 4.2966e-04\n",
      "Epoch 47/100\n",
      "86/86 [==============================] - 0s 905us/step - loss: 4.2194e-04\n",
      "Epoch 48/100\n",
      "86/86 [==============================] - 0s 834us/step - loss: 4.0622e-04\n",
      "Epoch 49/100\n",
      "86/86 [==============================] - 0s 780us/step - loss: 3.7028e-04\n",
      "Epoch 50/100\n",
      "86/86 [==============================] - 0s 667us/step - loss: 4.0887e-04\n",
      "Epoch 51/100\n",
      "86/86 [==============================] - 0s 651us/step - loss: 4.1450e-04\n",
      "Epoch 52/100\n",
      "86/86 [==============================] - 0s 679us/step - loss: 4.2552e-04\n",
      "Epoch 53/100\n",
      "86/86 [==============================] - 0s 682us/step - loss: 3.8384e-04\n",
      "Epoch 54/100\n",
      "86/86 [==============================] - 0s 635us/step - loss: 4.2345e-04\n",
      "Epoch 55/100\n",
      "86/86 [==============================] - 0s 633us/step - loss: 4.1654e-04\n",
      "Epoch 56/100\n",
      "86/86 [==============================] - 0s 881us/step - loss: 3.8701e-04\n",
      "Epoch 57/100\n",
      "86/86 [==============================] - 0s 870us/step - loss: 3.6627e-04\n",
      "Epoch 58/100\n",
      "86/86 [==============================] - 0s 961us/step - loss: 3.8274e-04\n",
      "Epoch 59/100\n",
      "86/86 [==============================] - 0s 857us/step - loss: 4.0510e-04\n",
      "Epoch 60/100\n",
      "86/86 [==============================] - 0s 878us/step - loss: 3.8551e-04\n",
      "Epoch 61/100\n",
      "86/86 [==============================] - 0s 870us/step - loss: 4.0244e-04\n",
      "Epoch 62/100\n",
      "86/86 [==============================] - 0s 845us/step - loss: 3.6450e-04\n",
      "Epoch 63/100\n",
      "86/86 [==============================] - 0s 908us/step - loss: 3.8350e-04\n",
      "Epoch 64/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 4.0842e-04\n",
      "Epoch 65/100\n",
      "86/86 [==============================] - 0s 722us/step - loss: 3.6294e-04\n",
      "Epoch 66/100\n",
      "86/86 [==============================] - 0s 738us/step - loss: 3.5617e-04\n",
      "Epoch 67/100\n",
      "86/86 [==============================] - 0s 713us/step - loss: 3.6305e-04\n",
      "Epoch 68/100\n",
      "86/86 [==============================] - 0s 668us/step - loss: 3.6917e-04\n",
      "Epoch 69/100\n",
      "86/86 [==============================] - 0s 777us/step - loss: 3.6975e-04\n",
      "Epoch 70/100\n",
      "86/86 [==============================] - 0s 818us/step - loss: 3.5800e-04\n",
      "Epoch 71/100\n",
      "86/86 [==============================] - 0s 840us/step - loss: 3.6557e-04\n",
      "Epoch 72/100\n",
      "86/86 [==============================] - 0s 810us/step - loss: 3.3970e-04\n",
      "Epoch 73/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 3.7783e-04\n",
      "Epoch 74/100\n",
      "86/86 [==============================] - 0s 784us/step - loss: 3.4326e-04\n",
      "Epoch 75/100\n",
      "86/86 [==============================] - 0s 862us/step - loss: 3.5732e-04\n",
      "Epoch 76/100\n",
      "86/86 [==============================] - 0s 848us/step - loss: 3.4635e-04\n",
      "Epoch 77/100\n",
      "86/86 [==============================] - 0s 982us/step - loss: 3.5157e-04\n",
      "Epoch 78/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 3.4398e-04\n",
      "Epoch 79/100\n",
      "86/86 [==============================] - 0s 807us/step - loss: 3.2945e-04\n",
      "Epoch 80/100\n",
      "86/86 [==============================] - 0s 662us/step - loss: 3.6786e-04\n",
      "Epoch 81/100\n",
      "86/86 [==============================] - 0s 642us/step - loss: 3.7075e-04\n",
      "Epoch 82/100\n",
      "86/86 [==============================] - 0s 666us/step - loss: 3.4112e-04\n",
      "Epoch 83/100\n",
      "86/86 [==============================] - 0s 731us/step - loss: 3.2711e-04\n",
      "Epoch 84/100\n",
      "86/86 [==============================] - 0s 916us/step - loss: 3.3337e-04\n",
      "Epoch 85/100\n",
      "86/86 [==============================] - 0s 844us/step - loss: 3.2423e-04\n",
      "Epoch 86/100\n",
      "86/86 [==============================] - 0s 965us/step - loss: 3.0296e-04\n",
      "Epoch 87/100\n",
      "86/86 [==============================] - 0s 958us/step - loss: 3.0779e-04\n",
      "Epoch 88/100\n",
      "86/86 [==============================] - 0s 980us/step - loss: 3.4127e-04\n",
      "Epoch 89/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 3.2045e-04\n",
      "Epoch 90/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 3.0768e-04\n",
      "Epoch 91/100\n",
      "86/86 [==============================] - 0s 934us/step - loss: 3.5837e-04\n",
      "Epoch 92/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 3.1623e-04\n",
      "Epoch 93/100\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 3.1577e-04\n",
      "Epoch 94/100\n",
      "86/86 [==============================] - 0s 784us/step - loss: 3.3922e-04\n",
      "Epoch 95/100\n",
      "86/86 [==============================] - 0s 703us/step - loss: 2.9508e-04\n",
      "Epoch 96/100\n",
      "86/86 [==============================] - 0s 806us/step - loss: 3.1491e-04\n",
      "Epoch 97/100\n",
      "86/86 [==============================] - 0s 764us/step - loss: 2.9262e-04\n",
      "Epoch 98/100\n",
      "86/86 [==============================] - 0s 681us/step - loss: 3.2348e-04\n",
      "Epoch 99/100\n",
      "86/86 [==============================] - 0s 956us/step - loss: 3.0231e-04\n",
      "Epoch 100/100\n",
      "86/86 [==============================] - 0s 919us/step - loss: 2.6838e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21a6803eee0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습시키기\n",
    "# 에포크 : 반복 횟수, batch_size : 한번에 학습할 사이즈 , verbose 1 : 학습의 진행 결과를 에포크별로 간단히 알려줌\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 예측하기\n",
    "# 모델의 성능을 측정하려면 실제 데이터를 예측한 값과 실제 데이터의 값의 차이를 봐야한다.\n",
    "# 그러므로 정규화를 거친 결과가 아닌 실제 확진자 수 데이터가 필요하다.\n",
    "# RNN 모델을 통해 나온 예측값을 정규화 되기 전으로 변환해 실제 값 또한 정규화되기 전의 값으로 변회시켜야함\n",
    "\n",
    "# X_train, X_tst -> RNN Model -> RNN -> 예측값     실제값 <= Scaler <- y_train, y_test\n",
    "\n",
    "trainPredict = model.predict(X_train) # 정규화된 값이 나옴\n",
    "testPredict = model.predic(X_test)\n",
    "\n",
    "# 실제 값으로 다시 바꾸기\n",
    "TrainPredict = scaler.inverse_transform(trainPredict)\n",
    "TestPredict = scaler.inverse_transform(testPredict)\n",
    "\n",
    "Y_train = scaler.inverse_transform([y_train])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7a68ff6df131d9ae7a0f60e2ded381d5113c51d2c465edd8572fa159edd1df8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
